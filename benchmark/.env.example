# Copy to `benchmark/.env` (or run from `benchmark/` and copy to `.env`) and fill in values.
#
# Provider-agnostic defaults (OpenAI-compatible Chat Completions):
SMI_PROVIDER=openai_compatible
SMI_API_KEY=REPLACE_ME
SMI_MODEL=REPLACE_ME
SMI_API_BASE_URL=https://api.openai.com/v1

# Optional knobs:
SMI_TEMPERATURE=0
SMI_MAX_TOKENS=2048

# Optional: provider-specific knobs.
#
# Z.AI supports a `thinking` parameter (GLM-4.7). The Coding Plan endpoint enables
# “Preserved Thinking” by default; for our single-turn benchmark calls we usually
# want to clear it.
#
# SMI_THINKING=enabled|disabled
# SMI_CLEAR_THINKING=true|false
#
# Z.AI also supports Structured Output (JSON mode). We recommend enabling it so
# the model reliably returns parseable JSON.
#
# SMI_RESPONSE_FORMAT=json_object

# If your provider is not OpenAI, set SMI_API_BASE_URL to that provider’s
# OpenAI-compatible base URL (it must support POST /chat/completions).
#
# Z.AI example (GLM-4.7):
# - If you have a Z.AI “Model API” resource package, use:
#   SMI_API_BASE_URL=https://api.z.ai/api/paas/v4
# - If you have a Z.AI “GLM Coding Plan” subscription, use the dedicated coding endpoint:
#   SMI_API_BASE_URL=https://api.z.ai/api/coding/paas/v4
# SMI_MODEL=glm-4.7
# SMI_THINKING=enabled
# SMI_CLEAR_THINKING=true
# SMI_RESPONSE_FORMAT=json_object
