# Copy to `benchmark/.env` (or run from `benchmark/` and copy to `.env`) and fill in values.
#
# Provider-agnostic defaults (OpenAI-compatible Chat Completions):
SMI_PROVIDER=openai_compatible
SMI_API_KEY=REPLACE_ME
SMI_MODEL=REPLACE_ME
SMI_API_BASE_URL=https://api.openai.com/v1

# Optional knobs:
SMI_TEMPERATURE=0
SMI_MAX_TOKENS=800
# Optional: for providers that support it (e.g., Z.AI), disable “Deep Thinking” to
# force the model to emit the final JSON in `message.content` (less verbosity and
# fewer token surprises).
# SMI_THINKING=disabled

# If your provider is not OpenAI, set SMI_API_BASE_URL to that provider’s
# OpenAI-compatible base URL (it must support POST /chat/completions).
#
# Z.AI example (GLM-4.7):
# - If you have a Z.AI “Model API” resource package, use:
#   SMI_API_BASE_URL=https://api.z.ai/api/paas/v4
# - If you have a Z.AI “GLM Coding Plan” subscription, use the dedicated coding endpoint:
#   SMI_API_BASE_URL=https://api.z.ai/api/coding/paas/v4
# SMI_MODEL=glm-4.7
# SMI_THINKING=disabled
